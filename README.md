# LAB1
В данной лабораторной работе используем пример полносвязной нейронной сети из 5 слоёв. 
Сначала добавляем Flatten-слой, разворачивающий нашу матрицу изображений в одномерную для дальнейшего поступления на Dense-слой.
Далее используем пять слоёв Dense: 2 слоя на 128 нейронов, еще 2 слоя на 64 нейрона и слой на 10 нейронов. 
Первые 4 Dense слоя используют активацию 'relu', последний - 'softmax'.

При обучении нейронной сети происходит 10 этапов обучения, при этом в цикле обучения используются:
-оптимизатор 'adam'
-функция потерь 'sparse_categorical_crossentropy'
-метрика 'accuracy'

Как ReLU сравнивает: ReLU является линейным (тождественным) для всех положительных значений и нулем для всех отрицательных значений.

Функция Softmax, замечательная функция активации, которая превращает числа или логиты в вероятности, которые равны единице. Функция Softmax выводит вектор, который представляет распределения вероятностей списка потенциальных результатов

При обучении нейронной сети происходит за 50 эпох

loss: 1.4366 - acc: 0.4827 - val_loss: 1.5164 - val_acc: 0.4581

Графики метрики точности и функции потерь:
![Image alt](https://github.com/PavelPoukh/LAB1/blob/master/acc.jpg)
![Image alt](https://github.com/PavelPoukh/LAB1/blob/master/loss.jpg)

Графики метрики точности и функции потерь на валидационной выборке:

![Image alt](https://github.com/PavelPoukh/LAB1/blob/master/val_acc.jpg)
![Image alt](https://github.com/PavelPoukh/LAB1/blob/master/val_loss.jpg)
